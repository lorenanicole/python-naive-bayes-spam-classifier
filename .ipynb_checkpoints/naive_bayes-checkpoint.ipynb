{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from decimal import Decimal\n",
    "import nltk\n",
    "import re\n",
    "import email.parser\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.5/site-packages')\n",
    "import lxml.html\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_body(filename):\n",
    "    with open(filename, encoding='utf-8', errors='replace') as f:\n",
    "        text = f.read()\n",
    "        msg = email.message_from_string(text)\n",
    "        payload = msg.get_payload()\n",
    "        if type(payload) == type(list()):\n",
    "            payload = payload[0]\n",
    "        plain_text_body_content = lxml.html.document_fromstring(str(payload)).text_content()\n",
    "        return plain_text_body_content\n",
    "\n",
    "class NaiveBayes(object):\n",
    "    def __init__(self, categories):\n",
    "        self.words = defaultdict(dict)\n",
    "        self.categories = self._create_categories(categories)\n",
    "        print('here is categories: {0}'.format(self.categories))\n",
    "        self.training_examples = 0\n",
    "        self.unique_words = set()\n",
    "\n",
    "    def _create_categories(self, categories):\n",
    "        categories = {category: {'total': 0, 'word_count': 0}\n",
    "                      for category in categories}\n",
    "        return categories\n",
    "\n",
    "    def train(self, category, text):\n",
    "        text = self._tokenize_text(text)  # TODO: stem words\n",
    "\n",
    "        self._increment_unique_word_count(text)  # Laplace Smoothing\n",
    "        self._increment_word_frequency(category, text)\n",
    "        self._increment_category_count(category)\n",
    "        self._increment_category_word_count(category, len(text))\n",
    "\n",
    "        self.training_examples += 1\n",
    "\n",
    "    def _tokenize_text(self, text):\n",
    "        text = re.findall(r\"[\\w']+\", text)\n",
    "        words = []\n",
    "        for word in text:\n",
    "            if word and word not in nltk.corpus.stopwords.words('english'):\n",
    "                words.append(word)\n",
    "        return words\n",
    "\n",
    "    def _increment_word_frequency(self, category, words):\n",
    "        for word in words:\n",
    "            if self.words[word].get(category):\n",
    "                self.words[word][category] += 1\n",
    "            else:\n",
    "                self.words[word][category] = 1\n",
    "\n",
    "    def _increment_unique_word_count(self, text):\n",
    "        self.unique_words = set(list(self.unique_words) + text)\n",
    "\n",
    "    def _increment_category_count(self, category):\n",
    "        self.categories[category]['total'] += 1\n",
    "\n",
    "    def _increment_category_word_count(self, category, number):\n",
    "        if self.categories[category].get('word_count'):\n",
    "            self.categories[category]['word_count'] += number\n",
    "        else:\n",
    "            self.categories[category]['word_count'] = number\n",
    "\n",
    "    def classify(self, text):\n",
    "        text = self._tokenize_text(text)\n",
    "\n",
    "        probabilities = {}\n",
    "        for cat, cat_data in self.categories:\n",
    "            print(cat, cat_data)\n",
    "            category_prob = self._get_category_probability(cat_data['total'])\n",
    "            predictors_likelihood = self._get_predictors_probability(cat, text)\n",
    "            probabilities[cat] = category_prob * predictors_likelihood\n",
    "\n",
    "        return 1 if probabilities[1] > probabilities[0] else 0\n",
    "\n",
    "    def _get_category_probability(self, count):\n",
    "        # Can make use of logarithm in lieu of Python's decimal object to avoid\n",
    "        # Floating point underflow\n",
    "        # e.g. return log(class_prior_prob)\n",
    "        return Decimal(float(count)) / Decimal(self.training_examples + len(self.categories.keys()))\n",
    "\n",
    "    def _get_predictors_probability(self, category, text):\n",
    "        word_count = self.categories[category]['word_count'] + len(self.unique_words)\n",
    "        likelihood = 1\n",
    "        for word in text:\n",
    "            if not self.words.get(word) or not self.words[word].get(category):\n",
    "                smoothed_freq = 1  # Laplace smoothing\n",
    "            else:\n",
    "                smoothed_freq = 1 + self.words[word][category]\n",
    "            likelihood *= Decimal(float(smoothed_freq)) / Decimal(word_count)\n",
    "            # floating point underflow!! EEE!\n",
    "            # http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "            # likelihood *= Decimal(float(self.words[word][category])) / Decimal(word_count)\n",
    "            # print category, log(predictor_likelihood)\n",
    "        return likelihood\n",
    "\n",
    "class SpamHamDetector(object):\n",
    "    def __init__(self, categories, path):\n",
    "        self.naive_bayes = NaiveBayes(categories)\n",
    "        self.path = path\n",
    "        self.classified_examples = dict()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        with open('{0}/labels.csv'.format(self.path), 'r') as labels_csv:\n",
    "            reader = csv.DictReader(labels_csv)\n",
    "            for row in reader:\n",
    "                label = (row['Prediction'])\n",
    "                filename = '%s/TR/TRAIN_%s.eml' % (path, row['Id'])\n",
    "                try:\n",
    "                    body = extract_body(filename)\n",
    "                    self.naive_bayes.train(int(label), body)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.info(\"Error training email %s: %s\", row['Id'], e.message)\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        all_ids = list(range(1, 2501))\n",
    "        random.shuffle(all_ids)\n",
    "        training_ids, labeling_ids = all_ids[:2250], all_ids[2250:]\n",
    "\n",
    "        with open('{0}/labels.csv'.format(self.path), 'r') as labels_csv:\n",
    "            reader = csv.DictReader(labels_csv)\n",
    "            for row in reader:\n",
    "                label = (row['Prediction'])\n",
    "                filename = '%s/TR/TRAIN_%s.eml' % (path, row['Id'])\n",
    "                if int(row['Id']) in training_ids:\n",
    "                    try:\n",
    "                        body = extract_body(filename)\n",
    "                        self.naive_bayes.train(int(label), body)\n",
    "                    except Exception as e:\n",
    "                        logger.info(\"Error training email %s: %s\", row['Id'], e.message)\n",
    "\n",
    "        correct, incorrect = 0, 0\n",
    "        with open('%s/labels.csv' % self.path, 'r') as labels_csv:\n",
    "            reader = csv.DictReader(labels_csv)\n",
    "            for row in reader:\n",
    "                label = (row['Prediction'])\n",
    "                filename = '%s/TR/TRAIN_%s.eml' % (path, row['Id'])\n",
    "                if int(row['Id']) in labeling_ids:\n",
    "                    try:\n",
    "                        test_body = extract_body(filename)\n",
    "                        result = self.naive_bayes.classify(test_body)\n",
    "                        if result == int(label):\n",
    "                            correct += 1\n",
    "                        else:\n",
    "                            incorrect += 1\n",
    "                    except Exception as e:\n",
    "                        logger.info(\"Error classifying email %s: %s\", row['Id'], e.message)\n",
    "        return self._calculate_results(correct, incorrect)\n",
    "\n",
    "    def classify(self, size):\n",
    "        counter = 1\n",
    "        test = self.path + '/TT/TEST_%s.eml'\n",
    "\n",
    "        while counter < size+1:\n",
    "            try:\n",
    "                test_body = extract_body(test % counter)\n",
    "                self.classified_examples[str(counter)] = str(self.naive_bayes.classify(test_body))\n",
    "            except Exception as e:\n",
    "                logger.info(\"Error classifying email %s: %s\", counter, e.message)\n",
    "            counter += 1\n",
    "\n",
    "        self._store_results()\n",
    "\n",
    "    def display_results(self):\n",
    "        spam = sum(1 for category in self.classified_examples.values() if category == '0')\n",
    "        ham = sum(1 for category in self.classified_examples.values() if category == '1')\n",
    "        return \"Spam Emails: %s\\nHam Emails: %s\\nSpam Percent: %s\\nHam Percent: %s\" \\\n",
    "               % (spam, ham, (float(spam) / len(self.classified_examples)),\n",
    "                  (float(ham) / len(self.classified_examples)))\n",
    "\n",
    "    def _calculate_results(self, correct, incorrect):\n",
    "        return \"correct %s, incorrect %s, performance measurement %s\" % (correct,\n",
    "                                                                         incorrect,\n",
    "                                                                         (float(correct) / (correct + incorrect)))\n",
    "\n",
    "    def _store_results(self):\n",
    "        with open('{}/results.csv'.format(self.path), 'w+') as resultscsv:\n",
    "            writer = csv.DictWriter(resultscsv, fieldnames=['id','Prediction'])\n",
    "            writer.writeheader()\n",
    "            for example_num, category in self.classified_examples.items():\n",
    "                writer.writerow({'id': example_num, 'Prediction': category})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting!\n",
      "here is categories: {0: {'word_count': 0, 'total': 0}, 1: {'word_count': 0, 'total': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"starting!\")\n",
    "path = '/Users/lorenamesa/Desktop/naive-bayes/naive-bayes' #os.path.dirname(__file__)\n",
    "detector = SpamHamDetector([0, 1], path)\n",
    "print(detector.train_and_evaluate())\n",
    "detector.train()\n",
    "print(\"done training!\")\n",
    "detector.classify(1827)\n",
    "print(\"done classifying!\")\n",
    "print(detector.display_results())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Porting Project from Python 2 -> Python 3\n",
    "\n",
    "- `email.parser` triggers `UnicodeError` - resolve via these [instructions](http://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s)\n",
    "- Error loading `lxml.html` resolve by commenting out Anaconda bin location in `~/.bash_profile`, and `brew install` directions found [here](http://stackoverflow.com/questions/23172384/lxml-runtime-error-reason-incompatible-library-version-etree-so-requires-vers) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
